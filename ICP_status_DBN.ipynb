{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "from dbn.tensorflow.models import UnsupervisedDBN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTokensOfOneFile( oneFileContent ):\n",
    "    stemmer_obj  = SnowballStemmer(\"english\")\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and\n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", oneFileContent)\n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Only inlcude words at least of length 3\n",
    "    valid_len_words = [w for w in meaningful_words if len(w) >= 3]\n",
    "\n",
    "    # convert words to utf\n",
    "    stemmed_words = [stemmer_obj.stem(token) for token in valid_len_words]\n",
    "    \n",
    "    #Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( stemmed_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveFileContent(fileNameParam):\n",
    "    str2ret=\"\"\n",
    "    for line_ in open(\"..//updated_data\" + fileNameParam, 'rU'):\n",
    "        li=line_.strip()\n",
    "        str2ret = str2ret + line_.rstrip()\n",
    "    return str2ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokensForTokenization(datasetParam):\n",
    "    completeCorpus    = [] ## a list of lists with tokens from defected and non defected files\n",
    "    with open(datasetParam, 'rU') as f:\n",
    "        reader_ = csv.reader(f)\n",
    "        next(reader_, None)\n",
    "        for row in reader_:\n",
    "            fileToRead   = row[0]\n",
    "            fileContentAsStr = giveFileContent(fileToRead)\n",
    "            filtered_str_from_one_file = processTokensOfOneFile(fileContentAsStr)\n",
    "            completeCorpus.append(filtered_str_from_one_file)       \n",
    "    return completeCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_numeric(label):\n",
    "    converted_label = np.empty(len(label), dtype=object) \n",
    "    for i in range(len(label)):\n",
    "        if label[i] == \"INSECURE\":\n",
    "            converted_label[i] = 1\n",
    "        elif label[i] == \"NEUTRAL\":\n",
    "            converted_label[i] = 0\n",
    "        else: \n",
    "            converted_label[i] = 2\n",
    "    converted_label = converted_label.astype('int')\n",
    "    return converted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(true_label, predicted_label):   \n",
    "    precision = recall = f1 = np.zeros(3, dtype=np.float32)\n",
    "    report = classification_report(true_label, predicted_label, digits=3)\n",
    "    precision = precision_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    recall = recall_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    f1 = f1_score(true_label, predicted_label, average=None, labels=[0,1])\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelEncoder(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source_code(data_file):     \n",
    "    unfilteredTokensFromFile = getTokensForTokenization(data_file)\n",
    "    return unfilteredTokensFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vector_of_nodes(docs):\n",
    "    #documents\n",
    "    #docs = ['if foo for bar car', 'foo for if bar']\n",
    "    \n",
    "    docs = [x.lower() for x in docs]\n",
    "  \n",
    "    # split documents to tokens\n",
    "    words = [doc.split(\" \") for doc in docs]\n",
    "    #print(\"All tokens: \", words)\n",
    "    \n",
    "    # find the length of vectors\n",
    "    max_len = np.max([len(x) for x in words])\n",
    "    print(\"Vector Length: \", max_len)\n",
    "    \n",
    "    # convert list of of token-lists to one flat list of tokens\n",
    "    flatten_words = list(itertools.chain.from_iterable(words))\n",
    "    #print(\"Flatten tokens: \", flatten_words)\n",
    "    \n",
    "    #fine all the unique tokens\n",
    "    unique_words = np.unique(flatten_words)\n",
    "    #print(\"Unique tokens: \", unique_words)\n",
    "    \n",
    "    # integer encode\n",
    "    encoded_docs = []\n",
    "    label_encoder = MyLabelEncoder()\n",
    "    label_encoder.fit(unique_words)\n",
    "    for doc in docs:\n",
    "        #print(doc)\n",
    "        words = doc.split(\" \")\n",
    "        #print(words)\n",
    "        integer_encoded = label_encoder.transform(words)\n",
    "        integer_encoded = np.pad(integer_encoded, (0, max_len - len(integer_encoded))) #padding with 0 to make fixed sized vectors\n",
    "        #print(integer_encoded)\n",
    "        encoded_docs.append(integer_encoded)\n",
    "    \n",
    "    return encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_classifier(clf):\n",
    "    dbn = UnsupervisedDBN(hidden_layers_structure=[256, 256],\n",
    "                              batch_size=64,\n",
    "                              learning_rate_rbm=0.06,\n",
    "                              n_epochs_rbm=1,\n",
    "                              activation_function='sigmoid',\n",
    "                              verbose =0)\n",
    "\n",
    "    classifier = Pipeline(steps=[('dbn', dbn), ('clf', clf)])  \n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(data, true_label):\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    # 10 fold cv\n",
    "    kf = KFold(n_splits=1, shuffle = True, random_state = 7)\n",
    "\n",
    "    cv_recall_DT = []\n",
    "    cv_precision_DT = []\n",
    "    cv_f1_DT = []\n",
    "\n",
    "    cv_recall_KNN = []\n",
    "    cv_precision_KNN = []\n",
    "    cv_f1_KNN = []\n",
    "\n",
    "    cv_recall_SVM = []\n",
    "    cv_precision_SVM = []\n",
    "    cv_f1_SVM = []\n",
    "\n",
    "    cv_recall_NB = []\n",
    "    cv_precision_NB = []\n",
    "    cv_f1_NB = []\n",
    "\n",
    "    cv_recall_RF = []\n",
    "    cv_precision_RF = []\n",
    "    cv_f1_RF = []\n",
    "    \n",
    "    cv_recall_DBN = []\n",
    "    cv_precision_DBN = []\n",
    "    cv_f1_DBN = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(data):        \n",
    "        train, test = data[train_index], data[test_index]\n",
    "        train_label, test_label = true_label[train_index], true_label[test_index]\n",
    "        \n",
    "        # Since DBN is effected by scale, we need to scale the features in the data before applying PCA\n",
    "        scaler = StandardScaler()\n",
    "        # Fit on training set only.\n",
    "        scaler.fit(train)\n",
    "        # Apply transform to both the training set and the test set.\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "        \n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                                      min_samples_split = 2, min_weight_fraction_leaf=0.0)\n",
    "        classifier = semantic_classifier(clf)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        print(\"CART fit time\", (end - start))\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        print(\"CART predict time\", (end - start))\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_DT.append(recall)\n",
    "        cv_precision_DT.append(precision)\n",
    "        cv_f1_DT.append(f1)\n",
    "\n",
    "        \n",
    "        clf = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "        classifier = semantic_classifier(clf)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        print(\"KNN fit time\", (end - start))\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        print(\"KNN predict time\", (end - start))\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_KNN.append(recall)\n",
    "        cv_precision_KNN.append(precision)\n",
    "        cv_f1_KNN.append(f1)\n",
    "\n",
    "        \n",
    "        clf = svm.SVC(gamma='auto', C = 20.0, kernel='rbf')\n",
    "        classifier = semantic_classifier(clf)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        print(\"SVM fit time\", (end - start))\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        print(\"SVM predict time\", (end - start))\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_SVM.append(recall)\n",
    "        cv_precision_SVM.append(precision)\n",
    "        cv_f1_SVM.append(f1)\n",
    "\n",
    "        \n",
    "        clf = GaussianNB()\n",
    "        classifier = semantic_classifier(clf)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        print(\"NB fit time\", (end - start))\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        print(\"NB predict time\", (end - start))\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_NB.append(recall)\n",
    "        cv_precision_NB.append(precision)\n",
    "        cv_f1_NB.append(f1)\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "        classifier = semantic_classifier(clf)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        print(\"RF fit time\", (end - start))\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        print(\"RF predict time\", (end - start))\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_RF.append(recall)\n",
    "        cv_precision_RF.append(precision)\n",
    "        cv_f1_RF.append(f1)\n",
    "        \n",
    "        clf = SupervisedDBNClassification(hidden_layers_structure = [256, 256],\n",
    "                    learning_rate_rbm=0.05,\n",
    "                    learning_rate=0.1,\n",
    "                    n_epochs_rbm=1,\n",
    "                    n_iter_backprop=1,\n",
    "                    batch_size=64,\n",
    "                    activation_function='relu',\n",
    "                    dropout_p=0.2,\n",
    "                    verbose=0)  \n",
    "        classifier = semantic_classifier(clf)\n",
    "        start = time.perf_counter()\n",
    "        classifier.fit(train, train_label)\n",
    "        end = time.perf_counter()\n",
    "        print(\"DBN fit time\", (end - start))\n",
    "        start = time.perf_counter()\n",
    "        predicted_label = classifier.predict(test)\n",
    "        end = time.perf_counter()\n",
    "        print(\"DBN predict time\", (end - start))\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall_DBN.append(recall)\n",
    "        cv_precision_DBN.append(precision)\n",
    "        cv_f1_DBN.append(f1)\n",
    "\n",
    "        \n",
    "    recall_DT = np.mean(cv_recall_DT, axis= 0)\n",
    "    precision_DT = np.mean(cv_precision_DT, axis= 0)\n",
    "    f1_DT = np.mean(cv_f1_DT, axis= 0)\n",
    "\n",
    "    recall_KNN = np.mean(cv_recall_KNN, axis= 0)\n",
    "    precision_KNN = np.mean(cv_precision_KNN, axis= 0)\n",
    "    f1_KNN = np.mean(cv_f1_KNN, axis= 0)\n",
    "\n",
    "    recall_SVM = np.mean(cv_recall_SVM, axis= 0)\n",
    "    precision_SVM = np.mean(cv_precision_SVM, axis= 0)\n",
    "    f1_SVM =  np.mean(cv_f1_SVM, axis= 0)\n",
    "\n",
    "    recall_NB = np.mean(cv_recall_NB, axis= 0)\n",
    "    precision_NB = np.mean(cv_precision_NB, axis= 0)\n",
    "    f1_NB = np.mean(cv_f1_NB, axis= 0)\n",
    "\n",
    "    recall_RF = np.mean(cv_recall_RF, axis= 0)\n",
    "    precision_RF = np.mean(cv_precision_RF, axis= 0)\n",
    "    f1_RF = np.mean(cv_f1_RF, axis= 0)\n",
    "    \n",
    "    recall_DBN = np.mean(cv_recall_DBN, axis= 0)\n",
    "    precision_DBN = np.mean(cv_precision_DBN, axis= 0)\n",
    "    f1_DBN = np.mean(cv_f1_DBN, axis= 0)\n",
    "    \n",
    "    return recall_DT, precision_DT, f1_DT, recall_KNN, precision_KNN, f1_KNN, recall_SVM, precision_SVM,\\\n",
    "    f1_SVM, recall_NB, precision_NB, f1_NB, recall_RF, precision_RF, f1_RF, recall_DBN, precision_DBN, f1_DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_file_test(train, test, train_label, test_label):\n",
    "    \n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    \n",
    "    # Since DBN is effected by scale, we need to scale the features in the data before applying PCA\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(train)\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "        \n",
    "        \n",
    "    clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                                      min_samples_split = 2, min_weight_fraction_leaf=0.0)\n",
    "    classifier = semantic_classifier(clf)\n",
    "    classifier.fit(train, train_label)\n",
    "    predicted_label = classifier.predict(test)\n",
    "    recall_DT, precision_DT, f1_DT = measure_performance(test_label, predicted_label)    \n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "    classifier = semantic_classifier(clf)\n",
    "    classifier.fit(train, train_label)\n",
    "    predicted_label = classifier.predict(test)\n",
    "    recall_KNN, precision_KNN, f1_KNN = measure_performance(test_label, predicted_label)\n",
    " \n",
    "    clf = svm.SVC(gamma='auto', C = 20.0, kernel='rbf')\n",
    "    classifier = semantic_classifier(clf)\n",
    "    classifier.fit(train, train_label)\n",
    "    predicted_label = classifier.predict(test)\n",
    "    recall_SVM, precision_SVM, f1_SVM  = measure_performance(test_label, predicted_label)\n",
    "\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    classifier = semantic_classifier(clf)\n",
    "    classifier.fit(train, train_label)\n",
    "    predicted_label = classifier.predict(test)\n",
    "    recall_NB, precision_NB, f1_NB = measure_performance(test_label, predicted_label)\n",
    "\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=10, criterion='gini')\n",
    "    classifier = semantic_classifier(clf)\n",
    "    classifier.fit(train, train_label)\n",
    "    predicted_label = classifier.predict(test)\n",
    "    recall_RF, precision_RF, f1_RF = measure_performance(test_label, predicted_label)\n",
    "    \n",
    "    \n",
    "    clf = SupervisedDBNClassification(hidden_layers_structure = [256, 256],\n",
    "                    learning_rate_rbm=0.05,\n",
    "                    learning_rate=0.1,\n",
    "                    n_epochs_rbm=10,\n",
    "                    n_iter_backprop=10,\n",
    "                    batch_size=64,\n",
    "                    activation_function='relu',\n",
    "                    dropout_p=0.2)  \n",
    "    classifier = semantic_classifier(clf)\n",
    "    classifier.fit(train, train_label)\n",
    "    predicted_label = classifier.predict(test)\n",
    "    recall_DBN, precision_DBN, f1_DBN = measure_performance(test_label, predicted_label)\n",
    "\n",
    "    return recall_DT, precision_DT, f1_DT, recall_KNN, precision_KNN, f1_KNN, recall_SVM, precision_SVM,\\\n",
    "    f1_SVM, recall_NB, precision_NB, f1_NB, recall_RF, precision_RF, f1_RF, recall_DBN, precision_DBN, f1_DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_test(data, true_label, train_data, test_data, train_label, test_label, test_name):\n",
    "    \n",
    "    repeated_recall_DT = []\n",
    "    repeated_precision_DT = []\n",
    "    repeated_f1_DT = []\n",
    "\n",
    "    repeated_recall_KNN = []\n",
    "    repeated_precision_KNN = []\n",
    "    repeated_f1_KNN = []\n",
    "\n",
    "    repeated_recall_SVM = []\n",
    "    repeated_precision_SVM = []\n",
    "    repeated_f1_SVM = []\n",
    "\n",
    "    repeated_recall_NB = []\n",
    "    repeated_precision_NB = []\n",
    "    repeated_f1_NB = []\n",
    "\n",
    "    repeated_recall_RF = []\n",
    "    repeated_precision_RF = []\n",
    "    repeated_f1_RF = []\n",
    "    \n",
    "    repeated_recall_DBN = []\n",
    "    repeated_precision_DBN = []\n",
    "    repeated_f1_DBN = []\n",
    "    \n",
    "    recall_DT= precision_DT= f1_DT= recall_KNN= precision_KNN= f1_KNN= recall_SVM= precision_SVM= f1_SVM\\\n",
    "    = recall_NB= precision_NB= f1_NB= recall_RF= precision_RF= f1_RF = recall_DBN= precision_DBN= f1_DBN =0\n",
    "    \n",
    "    for i in range(10):\n",
    "        if test_name == \"k_fold\":\n",
    "            recall_DT, precision_DT, f1_DT, recall_KNN, precision_KNN, f1_KNN, recall_SVM, precision_SVM, f1_SVM,\\\n",
    "            recall_NB, precision_NB, f1_NB, recall_RF, precision_RF, f1_RF, recall_DBN, precision_DBN, f1_DBN = kfold_cv(data, true_label)\n",
    "        else: \n",
    "            recall_DT, precision_DT, f1_DT, recall_KNN, precision_KNN, f1_KNN, recall_SVM, precision_SVM, f1_SVM,\\\n",
    "            recall_NB, precision_NB, f1_NB, recall_RF, precision_RF, f1_RF, recall_DBN, precision_DBN, f1_DBN = different_file_test(train_data, test_data, train_label, test_label)\n",
    "        \n",
    "        repeated_recall_DT.append(recall_DT)\n",
    "        repeated_precision_DT.append(precision_DT)\n",
    "        repeated_f1_DT.append(f1_DT)\n",
    "\n",
    "        repeated_recall_KNN.append(recall_KNN)\n",
    "        repeated_precision_KNN.append(precision_KNN)\n",
    "        repeated_f1_KNN.append(f1_KNN)\n",
    "\n",
    "        repeated_recall_SVM.append(recall_SVM)\n",
    "        repeated_precision_SVM.append(precision_SVM)\n",
    "        repeated_f1_SVM.append(f1_SVM)\n",
    "\n",
    "        repeated_recall_NB.append(recall_NB)\n",
    "        repeated_precision_NB.append(precision_NB)\n",
    "        repeated_f1_NB.append(f1_NB)\n",
    "\n",
    "        repeated_recall_RF.append(recall_RF)\n",
    "        repeated_precision_RF.append(precision_RF)\n",
    "        repeated_f1_RF.append(f1_RF)\n",
    "        \n",
    "        repeated_recall_DBN.append(recall_DBN)\n",
    "        repeated_precision_DBN.append(precision_DBN)\n",
    "        repeated_f1_DBN.append(f1_DBN)\n",
    "        \n",
    "    print(\"-------DT-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_DT, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_DT, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_DT, axis= 0))\n",
    "\n",
    "    print(\"-------KNN-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_KNN, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_KNN, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_KNN, axis= 0))\n",
    "\n",
    "    print(\"-------SVM-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_SVM, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_SVM, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_SVM, axis= 0))\n",
    "\n",
    "    print(\"-------NB-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_NB, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_NB, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_NB, axis= 0))\n",
    "\n",
    "    print(\"-------RF-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_RF, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_RF, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_RF, axis= 0))\n",
    "    \n",
    "    print(\"-------DBN-------\")\n",
    "    print(\"Recall:\", np.median(repeated_recall_DBN, axis= 0))\n",
    "    print(\"Precision:\", np.median(repeated_precision_DBN, axis= 0))\n",
    "    print(\"f1 score:\", np.median(repeated_f1_DBN, axis= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_files, true_label, train_data, test_data, train_label, test_label = [], [], [], [], [], [] # necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = '..//updated_data//RAW_DATASETS//COLOCATED_MOZILLA.csv'\n",
    "print(\"Data set: COLOCATED_MOZILLA.csv\")\n",
    "\n",
    "parsed_files = parse_source_code(data)\n",
    "file_df = pd.read_csv(data) \n",
    "print(\"Total File: \", file_df.shape[0])\n",
    "\n",
    "true_label = file_df['ICP_STATUS']\n",
    "true_label = convert_label_to_numeric(true_label)\n",
    "\n",
    "encoded_files = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_data, test_data, train_label, test_label, \"k_fold\") #repeat kfold 10 times and report avarage performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '..//updated_data//RAW_DATASETS//COLOCATED_OPENSTACK.csv'\n",
    "print(\"Data set: COLOCATED_OPENSTACK.csv\")\n",
    "\n",
    "parsed_files = parse_source_code(data)\n",
    "file_df = pd.read_csv(data) \n",
    "print(\"Total File: \", file_df.shape[0])\n",
    "\n",
    "true_label = file_df['ICP_STATUS']\n",
    "true_label = convert_label_to_numeric(true_label)\n",
    "\n",
    "encoded_files = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_data, test_data, train_label, test_label, \"k_fold\") #repeat kfold 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '..//updated_data//RAW_DATASETS//COLOCATED_WIKIMEDIA.csv'\n",
    "print(\"Data set: COLOCATED_WIKIMEDIA.csv\")\n",
    "\n",
    "parsed_files = parse_source_code(data)\n",
    "file_df = pd.read_csv(data) \n",
    "print(\"Total File: \", file_df.shape[0])\n",
    "\n",
    "true_label = file_df['ICP_STATUS']\n",
    "true_label = convert_label_to_numeric(true_label)\n",
    "\n",
    "encoded_files = encode_vector_of_nodes(parsed_files)\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_data, test_data, train_label, test_label, \"k_fold\") #repeat kfold 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '..//updated_data//RAW_DATASETS//COLOCATED_MOZILLA.csv'\n",
    "print(\"Train Data: COLOCATED_MOZILLA.csv\")\n",
    "parsed_train = parse_source_code(train_data)\n",
    "train_file_df = pd.read_csv(train_data) \n",
    "print(\"Total File: \", train_file_df.shape[0])\n",
    "train_label = train_file_df['ICP_STATUS']\n",
    "train_label = convert_label_to_numeric(train_label)\n",
    "\n",
    "test_data = '..//updated_data//RAW_DATASETS//COLOCATED_OPENSTACK.csv'\n",
    "print(\"Test Data: COLOCATED_OPENSTACK.csv\")\n",
    "parsed_test = parse_source_code(test_data)\n",
    "test_file_df = pd.read_csv(test_data) \n",
    "print(\"Total File: \", test_file_df.shape[0])\n",
    "test_label = test_file_df['ICP_STATUS']\n",
    "test_label = convert_label_to_numeric(test_label)\n",
    "\n",
    "\n",
    "parsed_file = parsed_train + parsed_test\n",
    "encoded_file = encode_vector_of_nodes(parsed_file)\n",
    "train_encoded = encoded_file[:train_file_df.shape[0]]\n",
    "test_encoded = encoded_file[train_file_df.shape[0]:]\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_encoded, test_encoded, train_label, test_label, \"different_file_test\") # repeat cross test 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '..//updated_data//RAW_DATASETS//COLOCATED_MOZILLA.csv'\n",
    "print(\"Train Data: COLOCATED_MOZILLA.csv\")\n",
    "parsed_train = parse_source_code(train_data)\n",
    "train_file_df = pd.read_csv(train_data) \n",
    "print(\"Total File: \", train_file_df.shape[0])\n",
    "train_label = train_file_df['ICP_STATUS']\n",
    "train_label = convert_label_to_numeric(train_label)\n",
    "\n",
    "\n",
    "test_data = '..//updated_data//RAW_DATASETS//COLOCATED_WIKIMEDIA.csv'\n",
    "print(\"Test Data: COLOCATED_WIKIMEDIA.csv\")\n",
    "parsed_test = parse_source_code(test_data)\n",
    "test_file_df = pd.read_csv(test_data) \n",
    "print(\"Total File: \", test_file_df.shape[0])\n",
    "test_label = test_file_df['ICP_STATUS']\n",
    "test_label = convert_label_to_numeric(test_label)\n",
    "\n",
    "\n",
    "parsed_file = parsed_train + parsed_test\n",
    "encoded_file = encode_vector_of_nodes(parsed_file)\n",
    "train_encoded = encoded_file[:train_file_df.shape[0]]\n",
    "test_encoded = encoded_file[train_file_df.shape[0]:]\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_encoded, test_encoded, train_label, test_label, \"different_file_test\") # repeat cross test 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '..//updated_data//RAW_DATASETS//COLOCATED_OPENSTACK.csv'\n",
    "print(\"Train Data: COLOCATED_OPENSTACK.csv\")\n",
    "parsed_train = parse_source_code(train_data)\n",
    "train_file_df = pd.read_csv(train_data) \n",
    "print(\"Total File: \", train_file_df.shape[0])\n",
    "train_label = train_file_df['ICP_STATUS']\n",
    "train_label = convert_label_to_numeric(train_label)\n",
    "\n",
    "\n",
    "test_data = '..//updated_data//RAW_DATASETS//COLOCATED_MOZILLA.csv'\n",
    "print(\"Test Data: COLOCATED_MOZILLA.csv\")\n",
    "parsed_test = parse_source_code(test_data)\n",
    "test_file_df = pd.read_csv(test_data) \n",
    "print(\"Total File: \", test_file_df.shape[0])\n",
    "test_label = test_file_df['ICP_STATUS']\n",
    "test_label = convert_label_to_numeric(test_label)\n",
    "\n",
    "\n",
    "parsed_file = parsed_train + parsed_test\n",
    "encoded_file = encode_vector_of_nodes(parsed_file)\n",
    "train_encoded = encoded_file[:train_file_df.shape[0]]\n",
    "test_encoded = encoded_file[train_file_df.shape[0]:]\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_encoded, test_encoded, train_label, test_label, \"different_file_test\") # repeat cross test 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '..//updated_data//RAW_DATASETS//COLOCATED_OPENSTACK.csv'\n",
    "print(\"Train Data: COLOCATED_OPENSTACK.csv\")\n",
    "parsed_train = parse_source_code(train_data)\n",
    "train_file_df = pd.read_csv(train_data) \n",
    "print(\"Total File: \", train_file_df.shape[0])\n",
    "train_label = train_file_df['ICP_STATUS']\n",
    "train_label = convert_label_to_numeric(train_label)\n",
    "\n",
    "\n",
    "test_data = '..//updated_data//RAW_DATASETS//COLOCATED_WIKIMEDIA.csv'\n",
    "print(\"Test Data: COLOCATED_WIKIMEDIA.csv\")\n",
    "parsed_test = parse_source_code(test_data)\n",
    "test_file_df = pd.read_csv(test_data) \n",
    "print(\"Total File: \", test_file_df.shape[0])\n",
    "test_label = test_file_df['ICP_STATUS']\n",
    "test_label = convert_label_to_numeric(test_label)\n",
    "\n",
    "\n",
    "parsed_file = parsed_train + parsed_test\n",
    "encoded_file = encode_vector_of_nodes(parsed_file)\n",
    "train_encoded = encoded_file[:train_file_df.shape[0]]\n",
    "test_encoded = encoded_file[train_file_df.shape[0]:]\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_encoded, test_encoded, train_label, test_label, \"different_file_test\") # repeat cross test 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '..//updated_data//RAW_DATASETS//COLOCATED_WIKIMEDIA.csv'\n",
    "print(\"Train Data: COLOCATED_WIKIMEDIA.csv\")\n",
    "parsed_train = parse_source_code(train_data)\n",
    "train_file_df = pd.read_csv(train_data) \n",
    "print(\"Total File: \", train_file_df.shape[0])\n",
    "train_label = train_file_df['ICP_STATUS']\n",
    "train_label = convert_label_to_numeric(train_label)\n",
    "\n",
    "\n",
    "test_data = '..//updated_data//RAW_DATASETS//COLOCATED_MOZILLA.csv'\n",
    "print(\"Test Data: COLOCATED_MOZILLA.csv\")\n",
    "parsed_test = parse_source_code(test_data)\n",
    "test_file_df = pd.read_csv(test_data) \n",
    "print(\"Total File: \", test_file_df.shape[0])\n",
    "test_label = test_file_df['ICP_STATUS']\n",
    "test_label = convert_label_to_numeric(test_label)\n",
    "\n",
    "\n",
    "parsed_file = parsed_train + parsed_test\n",
    "encoded_file = encode_vector_of_nodes(parsed_file)\n",
    "train_encoded = encoded_file[:train_file_df.shape[0]]\n",
    "test_encoded = encoded_file[train_file_df.shape[0]:]\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_encoded, test_encoded, train_label, test_label, \"different_file_test\") # repeat cross test 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '..//updated_data//RAW_DATASETS//COLOCATED_WIKIMEDIA.csv'\n",
    "print(\"Train Data: COLOCATED_WIKIMEDIA.csv\")\n",
    "parsed_train = parse_source_code(train_data)\n",
    "train_file_df = pd.read_csv(train_data) \n",
    "print(\"Total File: \", train_file_df.shape[0])\n",
    "train_label = train_file_df['ICP_STATUS']\n",
    "train_label = convert_label_to_numeric(train_label)\n",
    "\n",
    "\n",
    "test_data = '..//updated_data//RAW_DATASETS//COLOCATED_OPENSTACK.csv'\n",
    "print(\"Test Data: COLOCATED_OPENSTACK.csv\")\n",
    "parsed_test = parse_source_code(test_data)\n",
    "test_file_df = pd.read_csv(test_data) \n",
    "print(\"Total File: \", test_file_df.shape[0])\n",
    "test_label = test_file_df['ICP_STATUS']\n",
    "test_label = convert_label_to_numeric(test_label)\n",
    "\n",
    "\n",
    "parsed_file = parsed_train + parsed_test\n",
    "encoded_file = encode_vector_of_nodes(parsed_file)\n",
    "train_encoded = encoded_file[:train_file_df.shape[0]]\n",
    "test_encoded = encoded_file[train_file_df.shape[0]:]\n",
    "\n",
    "repeated_test(encoded_files, true_label, train_encoded, test_encoded, train_label, test_label, \"different_file_test\") # repeat cross test 10 times and report avarage performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
