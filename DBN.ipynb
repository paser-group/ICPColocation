{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: hyperparameter optimization, tesing on different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbn import SupervisedDBNClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTokensOfOneFile( oneFileContent ):\n",
    "    stemmer_obj  = SnowballStemmer(\"english\")\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and\n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", oneFileContent)\n",
    "    \n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Only inlcude words at least of length 3\n",
    "    valid_len_words = [w for w in meaningful_words if len(w) >= 3]\n",
    "\n",
    "    # convert words to utf\n",
    "    stemmed_words = [stemmer_obj.stem(token) for token in valid_len_words]\n",
    "    \n",
    "    #Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( stemmed_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveFileContent(fileNameParam):\n",
    "    str2ret=\"\"\n",
    "    for line_ in open(\"..//updated_data\" + fileNameParam, 'rU'):\n",
    "        li=line_.strip()\n",
    "        str2ret = str2ret + line_.rstrip()\n",
    "    return str2ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokensForTokenization(datasetParam):\n",
    "    completeCorpus    = [] ## a list of lists with tokens from defected and non defected files\n",
    "    with open(datasetParam, 'rU') as f:\n",
    "        reader_ = csv.reader(f)\n",
    "        next(reader_, None)\n",
    "        for row in reader_:\n",
    "            fileToRead   = row[0]\n",
    "            fileContentAsStr = giveFileContent(fileToRead)\n",
    "            filtered_str_from_one_file = processTokensOfOneFile(fileContentAsStr)\n",
    "            completeCorpus.append(filtered_str_from_one_file)       \n",
    "    return completeCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_numeric(label):\n",
    "    converted_label = np.empty(len(label), dtype=object) \n",
    "    for i in range(len(label)):\n",
    "        if label[i] == \"ONLY_ONE\":\n",
    "            converted_label[i] = 1\n",
    "        elif label[i] == \"NEUTRAL\":\n",
    "            converted_label[i] = 0\n",
    "        else: \n",
    "            converted_label[i] = 2\n",
    "    converted_label = converted_label.astype('int')\n",
    "    return converted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(true_label, predicted_label):   \n",
    "    report = classification_report(true_label, predicted_label, digits=3)\n",
    "    recall = recall_score(true_label, predicted_label, average=\"macro\")\n",
    "    precision = precision_score(true_label, predicted_label, average=\"macro\")\n",
    "    f1 = f1_score(true_label, predicted_label, average=\"macro\")\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelEncoder(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source_code(data_file):     \n",
    "    unfilteredTokensFromFile = getTokensForTokenization(data_file)\n",
    "    return unfilteredTokensFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vector_of_nodes(docs):\n",
    "    #documents\n",
    "    #docs = ['if foo for bar car', 'foo for if bar']\n",
    "    \n",
    "    docs = [x.lower() for x in docs]\n",
    "  \n",
    "    # split documents to tokens\n",
    "    words = [doc.split(\" \") for doc in docs]\n",
    "    #print(\"All tokens: \", words)\n",
    "    \n",
    "    # find the length of vectors\n",
    "    max_len = np.max([len(x) for x in words])\n",
    "    print(\"Vector Length: \", max_len)\n",
    "    \n",
    "    # convert list of of token-lists to one flat list of tokens\n",
    "    flatten_words = list(itertools.chain.from_iterable(words))\n",
    "    #print(\"Flatten tokens: \", flatten_words)\n",
    "    \n",
    "    #fine all the unique tokens\n",
    "    unique_words = np.unique(flatten_words)\n",
    "    #print(\"Unique tokens: \", unique_words)\n",
    "    \n",
    "    # integer encode\n",
    "    encoded_docs = []\n",
    "    label_encoder = MyLabelEncoder()\n",
    "    label_encoder.fit(unique_words)\n",
    "    for doc in docs:\n",
    "        #print(doc)\n",
    "        words = doc.split(\" \")\n",
    "        #print(words)\n",
    "        integer_encoded = label_encoder.transform(words)\n",
    "        integer_encoded = np.pad(integer_encoded, (0, max_len - len(integer_encoded))) #padding with 0 to make fixed sized vectors\n",
    "        #print(integer_encoded)\n",
    "        encoded_docs.append(integer_encoded)\n",
    "    \n",
    "    return encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_using_dbn(data, true_label):\n",
    "    \n",
    "    data = np.array(data)\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    \n",
    "    # 10 fold cv\n",
    "    kf = KFold(n_splits=10, shuffle = True, random_state = 7)\n",
    "\n",
    "    cv_recall = []\n",
    "    cv_precision = []\n",
    "    cv_f1 = []\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        train, test = data[train_index], data[test_index]\n",
    "        train_label, test_label = true_label[train_index], true_label[test_index]\n",
    "\n",
    "        # Since DBN is effected by scale, we need to scale the features in the data before applying PCA\n",
    "        scaler = StandardScaler()\n",
    "        # Fit on training set only.\n",
    "        scaler.fit(train)\n",
    "        # Apply transform to both the training set and the test set.\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "\n",
    "        dbn = SupervisedDBNClassification(hidden_layers_structure = [256, 256],\n",
    "                    learning_rate_rbm=0.05,\n",
    "                    learning_rate=0.001,\n",
    "                    n_epochs_rbm=1,\n",
    "                    n_iter_backprop=1,\n",
    "                    batch_size=32,\n",
    "                    activation_function='relu',\n",
    "                    dropout_p=0.2)\n",
    "        dbn.fit(train, train_label)\n",
    "        predicted_label = dbn.predict(test)\n",
    "        recall, precision, f1 = measure_performance(test_label, predicted_label)\n",
    "        cv_recall.append(recall)\n",
    "        cv_precision.append(precision)\n",
    "        cv_f1.append(f1)\n",
    "        \n",
    "    print(\"Recall:\", np.mean(cv_recall))\n",
    "    print(\"Precision:\", np.mean(cv_precision))\n",
    "    print(\"f1 score:\", np.mean(cv_f1))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '..//updated_data//RAW_DATASETS//COLOCATED_MOZILLA.csv'\n",
    "parsed_files = parse_source_code(data_file)\n",
    "\n",
    "file_df = pd.read_csv(data_file) \n",
    "print(\"Total File: \", file_df.shape[0])\n",
    "true_label = file_df['COLOCATED_STATUS']\n",
    "true_label = convert_label_to_numeric(true_label)\n",
    "file_names = np.unique(file_df['FILE_PATH'].tolist())\n",
    "\n",
    "encoded_files = encode_vector_of_nodes(parsed_files)\n",
    "prediction_using_dbn(encoded_files, true_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
